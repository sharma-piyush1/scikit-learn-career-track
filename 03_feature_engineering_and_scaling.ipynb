{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40ee4cc9",
      "metadata": {
        "id": "40ee4cc9"
      },
      "source": [
        "# 03 — Feature Engineering and Scaling\n",
        "\n",
        "Feature engineering improves model performance more than changing the model itself.\n",
        "\n",
        "In this notebook we:\n",
        "- Identify and remove low-variance (uninformative) features\n",
        "- Compare different feature scaling techniques\n",
        "- Introduce polynomial and interaction features for non-linear relationships\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bb6e6d4",
      "metadata": {
        "id": "1bb6e6d4"
      },
      "source": [
        "Step 1 — Reload and Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9fc4d6db",
      "metadata": {
        "id": "9fc4d6db"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df = df.drop(columns=[\"Cabin\", \"Ticket\", \"Name\", \"PassengerId\"])\n",
        "X = df.drop(columns=[\"Survived\"])\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 — Separate numerical and categorical columns"
      ],
      "metadata": {
        "id": "b85AuIl3f_qC"
      },
      "id": "b85AuIl3f_qC"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8f9fa069",
      "metadata": {
        "id": "8f9fa069"
      },
      "outputs": [],
      "source": [
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 — Standard Scaling vs Min-Max Scaling"
      ],
      "metadata": {
        "id": "UDnrb623gFjn"
      },
      "id": "UDnrb623gFjn"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "scaler_std = StandardScaler()\n",
        "scaler_minmax = MinMaxScaler()\n",
        "\n",
        "X_train_std = scaler_std.fit_transform(X_train[numeric_features])\n",
        "X_train_minmax = scaler_minmax.fit_transform(X_train[numeric_features])\n",
        "\n",
        "pd.DataFrame(X_train_std).head(), pd.DataFrame(X_train_minmax).head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rbMZfeSgFuR",
        "outputId": "28e3c9a9-e208-42ac-81a3-ced19d11ffa2"
      },
      "id": "6rbMZfeSgFuR",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(          0         1         2         3         4\n",
              " 0  0.829568       NaN -0.465084 -0.466183  0.513812\n",
              " 1 -0.370945       NaN -0.465084 -0.466183 -0.662563\n",
              " 2 -1.571457       NaN -0.465084 -0.466183  3.955399\n",
              " 3  0.829568 -0.815864 -0.465084  0.727782 -0.467874\n",
              " 4 -0.370945  0.082384  0.478335  0.727782 -0.115977,\n",
              "      0         1      2         3         4\n",
              " 0  1.0       NaN  0.000  0.000000  0.110272\n",
              " 1  0.5       NaN  0.000  0.000000  0.000000\n",
              " 2  0.0       NaN  0.000  0.000000  0.432884\n",
              " 3  1.0  0.220910  0.000  0.166667  0.018250\n",
              " 4  0.5  0.384267  0.125  0.166667  0.051237)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must visually see that:\n",
        "\n",
        "StandardScaler → centers around 0\n",
        "\n",
        "MinMaxScaler → rescales to 0 to 1\n",
        "\n",
        "This is not memorization.\n",
        "This is pattern recognition."
      ],
      "metadata": {
        "id": "QDmgpKSzgRmt"
      },
      "id": "QDmgpKSzgRmt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 — Low-Variance Feature Removal (optional but professional)"
      ],
      "metadata": {
        "id": "H3zlqHGggS_x"
      },
      "id": "H3zlqHGggS_x"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.0)\n",
        "selector.fit(X_train[numeric_features])\n",
        "\n",
        "numeric_features[selector.get_support()]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw-eMXKHgHrV",
        "outputId": "51ae8b9b-365c-4806-e7eb-0dcfd7e6137f"
      },
      "id": "Kw-eMXKHgHrV",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If some columns get removed → good.\n",
        "If none → also fine.\n",
        "You are learning analysis, not forcing results."
      ],
      "metadata": {
        "id": "p1etM-0CgbrK"
      },
      "id": "p1etM-0CgbrK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 — Polynomial Features (the real performance booster)"
      ],
      "metadata": {
        "id": "0RCCtERfgdM3"
      },
      "id": "0RCCtERfgdM3"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# First impute missing values\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X_train_num_imputed = imputer.fit_transform(X_train[numeric_features])\n",
        "\n",
        "# Then apply polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_num_imputed)\n",
        "\n",
        "X_train_poly.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1xGDa49gVj5",
        "outputId": "f4198f8a-0fb1-40b3-a05c-7fd6c6b5e54b"
      },
      "id": "s1xGDa49gVj5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(712, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point here is:\n",
        "\n",
        "Linear models become nonlinear with polynomial expansion\n",
        "\n",
        "But complexity increases fast → must use carefully"
      ],
      "metadata": {
        "id": "lD3jfkv5hQeq"
      },
      "id": "lD3jfkv5hQeq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 — Compare Model Performance (Standard vs Poly)"
      ],
      "metadata": {
        "id": "LBnmT_D4hSWr"
      },
      "id": "LBnmT_D4hSWr"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# 1) Without polynomial features\n",
        "num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", num_pipe, numeric_features),\n",
        "    (\"cat\", cat_pipe, categorical_features)\n",
        "])\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000))\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "pred1 = model.predict(X_test)\n",
        "print(\"Baseline Accuracy:\", accuracy_score(y_test, pred1))\n",
        "\n",
        "\n",
        "# 2) With polynomial interaction terms\n",
        "poly_num_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "preprocess_poly = ColumnTransformer([\n",
        "    (\"num\", poly_num_pipe, numeric_features),\n",
        "    (\"cat\", cat_pipe, categorical_features)\n",
        "])\n",
        "\n",
        "model_poly = Pipeline([\n",
        "    (\"preprocess\", preprocess_poly),\n",
        "    (\"clf\", LogisticRegression(max_iter=4000))\n",
        "])\n",
        "\n",
        "model_poly.fit(X_train, y_train)\n",
        "pred2 = model_poly.predict(X_test)\n",
        "print(\"Polynomial Accuracy:\", accuracy_score(y_test, pred2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxM6WlSNgfXn",
        "outputId": "74f1ebce-54ad-47e7-9fc1-3de287e8f313"
      },
      "id": "jxM6WlSNgfXn",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy: 0.8044692737430168\n",
            "Polynomial Accuracy: 0.8100558659217877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If polynomial accuracy decreases → overfitting sign → we will fix later with regularization and CV."
      ],
      "metadata": {
        "id": "2fEGW3XhiBQA"
      },
      "id": "2fEGW3XhiBQA"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RL6G3nnAh4pJ"
      },
      "id": "RL6G3nnAh4pJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}